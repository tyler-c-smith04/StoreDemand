# Filter rows where 'house_rules' mentions 'pets'
no_pets <- abnb %>%
filter(str_detect(house_rules, "no pets"))
view(no_pets)
install.packages('doParallel')
library(readr)
nfl_suspensions_data <- read_csv("Downloads/nfl-suspensions-data.csv")
View(nfl_suspensions_data)
library(tidvyerse)
nfl_suspensions <- read_csv("Downloads/nfl-suspensions-data.csv")
suspensions_per_year <- nfl_suspensions %>%
group_by(year) %>%
summarise(count = n())
print(suspensions_per_year)
getwd()
library(tidvyerse)
library(tidvyerse)
install.packages('tidyverser')
install.packages('tidyverse')
install.packages("tidyverse")
library(tidvyerse)
library(tidyverse)
library(vroom)
library(readr)
library(stringr)
abnb <- read_csv("/Users/tylersmith/Development/Projects/doterra/Airbnb_Open_Data.csv")
# Replace spaces with underscores in column names
names(abnb) <- gsub(" ", "_", names(abnb))
abnb <- abnb %>%
select(id, host_id, host_id, host_name, host_identity_verified, neighbourhood_group, neighbourhood, lat, long, room_type,
Construction_year, price) %>%
filter(host_identity_verified == 'verified')
# Transform the price column and convert Construction_year to factor
abnb <- abnb %>%
mutate(price = str_replace_all(price, "\\$", "") %>%    # Replace dollar signs
str_replace_all(",", "") %>%               # Replace commas
as.numeric()) %>%
mutate(Construction_year = as.factor(Construction_year))
# Transform the price column
abnb <- abnb %>%
mutate(price = str_replace_all(price, "\\$", "") %>%    # Replace dollar signs
str_replace_all(",", "") %>%               # Replace commas
as.numeric())
# Count NA's in columns
na_count <- colSums(is.na(abnb))
abnb <- abnb %>%
drop_na(neighbourhood_group, neighbourhood, lat, long)
# Check to make sure that the NA's are removed from the groups that I wanted
sum(is.na(abnb$neighbourhood_group))
sum(is.na(abnb$neighbourhood))
sum(is.na(abnb$lat))
sum(is.na(abnb$long))
unique(abnb$neighbourhood_group)
# Change misspelled values in neighbourhood_group to make sure they match the real value
abnb <- abnb %>%
mutate(neighbourhood_group = case_when(
neighbourhood_group == "brookln" ~ "Brooklyn",
neighbourhood_group == "manhatan" ~ "Manhattan",
TRUE ~ neighbourhood_group # This makes sure that other values are unchanged
))
unique(abnb$neighbourhood_group)
view(abnb)
view(abnb)
view(abnb2)
library(readr)
nfl_suspensions_data <- read_csv("Development/Projects/nfl-suspensions-data.csv")
View(nfl_suspensions_data)
nfl_suspensions <- read_csv("Development/Projects/nfl-suspensions-data.csv")
nfl_suspensions
nfl_suspensions %>%
filter(year == 2012)
library(tidyverse)
nfl_suspensions %>%
filter(year == 2012)
view(nfl_suspensions %>%
filter(year == 2012))
len(nfl_suspensions %>%
filter(year == 2012))
filter(year == 2012))
length(nfl_suspensions %>%
filter(year == 2012))
nfl_suspensions %>%
filter(year == 2012)
library(readr)
nfl_suspensions_data <- read_csv("Development/Projects/nfl-suspensions-data.csv")
View(nfl_suspensions_data)
library(tidyverse)
nfl_suspensions <- read_csv("Development/Projects/nfl-suspensions-data.csv")
view(nfl_suspensions)
all_years <- tibble(Year = 1986:2014)
nfl_data_complete <- all_years %>%
left_join(nfl_data, by = "Year") %>%
mutate(Suspensions = ifelse(is.na(Suspensions), 0, Suspensions))
nfl_data_complete <- all_years %>%
left_join(nfl_suspensions, by = "Year") %>%
mutate(Suspensions = ifelse(is.na(Suspensions), 0, Suspensions))
nfl_data_complete <- all_years %>%
left_join(nfl_suspensions, by = "year") %>%
mutate(Suspensions = ifelse(is.na(Suspensions), 0, Suspensions))
library(tidyverse)
nfl_suspensions <- read_csv("Development/Projects/nfl-suspensions-data.csv")
all_years <- tibble(Year = 1986:2015)
nfl_data_complete <- all_years %>%
left_join(nfl_suspensions, by = c("Year" = "Year")) %>%
mutate(Suspensions = ifelse(is.na(Suspensions), 0, Suspensions))
library(tidyverse)
nfl_suspensions <- read_csv("Development/Projects/nfl-suspensions-data.csv")
all_years <- tibble(Year = 1986:2015)
nfl_data_complete <- all_years %>%
left_join(nfl_suspensions, by = c("year" = "year")) %>%
mutate(Suspensions = ifelse(is.na(Suspensions), 0, Suspensions))
library(tidyverse)
nfl_suspensions <- read_csv("Development/Projects/nfl-suspensions-data.csv")
all_years <- tibble(year = 1986:2015)
nfl_data_complete <- all_years %>%
left_join(nfl_suspensions, by = c("year" = "year")) %>%
mutate(Suspensions = ifelse(is.na(Suspensions), 0, Suspensions))
library(tidyverse)
nfl_suspensions <- read_csv("Development/Projects/nfl-suspensions-data.csv")
all_years <- tibble(Year = 1986:2015)
nfl_suspensions_2 <- all_years %>%
left_join(nfl_suspensions, by = "year") %>%
group_by(year) %>%
summarize(suspensions_count = n())
library(tidyverse)
nfl_suspensions <- read_csv("Development/Projects/nfl-suspensions-data.csv")
all_years <- tibble(year = 1986:2015)
nfl_suspensions_2 <- all_years %>%
left_join(nfl_suspensions, by = "Year") %>%
group_by(year) %>%
summarize(suspensions_count = n())
library(tidyverse)
nfl_suspensions <- read_csv("Development/Projects/nfl-suspensions-data.csv")
all_years <- tibble(year = 1986:2015)
nfl_suspensions_2 <- all_years %>%
left_join(nfl_suspensions, by = "year") %>%
group_by(year) %>%
summarize(suspensions_count = n())
write.csv(nfl_suspensions_2, "nfl_supsensions_2.csv", row.names = FALSE)
view(nfl_suspensions_2)
nfl_suspensions_updated <- nfl_suspensions %>%
left_join(nfl_suspensions_2, by = "year")
view(nfl_suspensions_updated)
view(nfl_data_updated)
nfl_data_updated <- nfl_data %>%
full_join(nfl_suspensions_2, by = "year")
nfl_suspensions_updated <- nfl_suspensions %>%
full_join(nfl_suspensions_2, by = "year")
nfl_suspensions_updated$games[is.na(nfl_suspensions_updated$games)] <- 0
unique(nfl_suspensions_updated$Year)
unique(nfl_suspensions_updated$year)
install.packages('Rserve')
library(Rserve)
Rserve(args=“–no-save”)
library(Rserve)
Rserve(args = "--no-save")
setwd("~/Desktop/STAT348/StoreDemand")
# Import Libraries
library(tidyverse)
library(tidymodels)
library(modeltime)
library(timetk)
library(vroom)
library(forecast)
# Read in the data
item <- vroom::vroom("./train.csv")
itemTest <- vroom::vroom("./test.csv")
# Determine the number of stores and items
n.stores <- max(item$store)
n.items <- max(item$item)
it <- 0
# Double Loop over all store-item combos
for(s in 1:n.stores){
for(i in 1:n.items){
# Increment the progress bar
it <- it + 1
# Subset the data
train <- item %>%
filter(store == s, item == i) %>%
select(date, sales)
test <- itemTest %>%
filter(store == s, item == i) %>%
select(id, date)
# CV Split
cv_splits <- time_series_split(train, assess = "3 months", cumulative = TRUE)
# Define and Fit SARIMA Model
sarima_model <- auto.arima(training(cv_splits)$sales, seasonal = TRUE)
# Forecast using the fitted model
forecasted_values <- forecast(sarima_model, h = length(testing(cv_splits)$sales))
# Prepare the forecast for submission
forecast_dates <- testing(cv_splits)$date
preds <- data.frame(date = forecast_dates, sales = forecasted_values$mean) %>%
full_join(., y = test, by = "date") %>%
select(id, sales)
# If first store-item save, otherwise bind
if(it == 1){
all_preds <- preds
} else {
all_preds <- bind_rows(all_preds, preds)
}
}
}
# Arrange predictions and write to a file
all_preds <- all_preds %>% arrange(id)
vroom_write(x = all_preds, file = "./submission.csv", delim = ",")
# Import Libraries
library(tidyverse)
library(tidymodels)
library(modeltime)
library(timetk)
library(vroom)
library(forecast)
# Read in the data
item <- vroom::vroom("path_to_your_train.csv")
# Import Libraries
library(tidyverse)
library(tidymodels)
library(modeltime)
library(timetk)
library(vroom)
library(forecast)
# Read in the data
item <- vroom::vroom("./train.csv")
itemTest <- vroom::vroom("./test.csv")
# Convert date to Date type
item$date <- as.Date(item$date)
itemTest$date <- as.Date(itemTest$date)
# Determine the number of stores and items
n.stores <- max(item$store)
n.items <- max(item$item)
# Initialize an empty data frame to store predictions
all_preds <- data.frame()
# Loop over all store-item combos
for(s in 1:n.stores) {
for(i in 1:n.items) {
# Subset the data for the current store-item combination
train <- item %>% filter(store == s, item == i)
test <- itemTest %>% filter(store == s, item == i)
# Check if there are any missing sales values
if(any(is.na(train$sales))) {
next # Skip to the next iteration if there are missing values
}
# Fit the Exponential Smoothing model
ets_model <- ets(train$sales)
# Forecast for the required horizon
forecast_horizon <- nrow(test)
forecasted_values <- forecast(ets_model, h = forecast_horizon)
# Prepare the forecast for submission
preds <- data.frame(id = test$id, sales = forecasted_values$mean)
# Combine the predictions for each store-item combination
all_preds <- bind_rows(all_preds, preds)
}
}
# Write the predictions to a CSV file
write.csv(all_preds, "exponential_smoothing_predictions.csv", row.names = FALSE)
library(tidyverse)
library(tidymodels)
library(modeltime)
library(timetk)
library(vroom)
library(forecast)
# Read in the data
item <- vroom::vroom("./train.csv")
itemTest <- vroom::vroom("./test.csv")
# Convert date to Date type
item$date <- as.Date(item$date)
itemTest$date <- as.Date(itemTest$date)
# Determine the number of stores and items
n.stores <- max(item$store)
n.items <- max(item$item)
# Initialize an empty data frame to store predictions
all_preds <- data.frame()
# Loop over all store-item combos
for(s in 1:n.stores) {
for(i in 1:n.items) {
# Subset the data for the current store-item combination
train <- item %>% filter(store == s, item == i)
test <- itemTest %>% filter(store == s, item == i)
# Check if there are any missing sales values
if(any(is.na(train$sales))) {
next # Skip to the next iteration if there are missing values
}
# Fit the SARIMA model
sarima_model <- auto.arima(train$sales, seasonal = TRUE)
# Forecast for the required horizon
forecast_horizon <- nrow(test)
forecasted_values <- forecast(sarima_model, h = forecast_horizon)
# Prepare the forecast for submission
preds <- data.frame(id = test$id, sales = forecasted_values$mean)
# Combine the predictions for each store-item combination
all_preds <- bind_rows(all_preds, preds)
}
}
# Write the predictions to a CSV file
write.csv(all_preds, "sarima_predictions.csv", row.names = FALSE)
# Import Libraries
library(tidyverse)
library(tidymodels)
library(modeltime)
library(timetk)
library(vroom)
library(forecast)
# Read in the data
item <- vroom::vroom("/kaggle/input/demand-forecasting-kernels-only/train.csv")
# Import Libraries
library(tidyverse)
library(tidymodels)
library(modeltime)
library(timetk)
library(vroom)
library(forecast)
# Read in the data
item <- vroom::vroom("/.train.csv")
setwd("~/Desktop/STAT348/StoreDemand")
# Import Libraries
library(tidyverse)
library(tidymodels)
library(modeltime)
library(timetk)
library(vroom)
library(forecast)
# Read in the data
item <- vroom::vroom("/.train.csv")
# Import Libraries
library(tidyverse)
library(tidymodels)
library(modeltime)
library(timetk)
library(vroom)
library(forecast)
# Read in the data
item <- vroom::vroom("./train.csv")
itemTest <- vroom::vroom("./test.csv")
# How many stores and items?
n.stores <- max(item$store)
n.items <- max(item$item)
it <- 0
# Double Loop over all store-item combos
for(s in 1:n.stores){
for(i in 1:n.items){
# Increment the progress bar
it <- it + 1
# Subset the data
train <- item %>%
filter(store == s, item == i) %>%
select(date, sales)
test <- itemTest %>%
filter(store == s, item == i) %>%
select(id, date)
# CV Split
cv_splits <- time_series_split(train, assess = "3 months", cumulative = TRUE)
# Define and Fit Exponential Smoothing Model
ets_model <- ets(training(cv_splits)$sales)
# Forecast using the fitted model
forecasted_values <- forecast(ets_model, h = length(testing(cv_splits)$sales))
# Prepare the forecast for submission
forecast_dates <- testing(cv_splits)$date
preds <- data.frame(date = forecast_dates, sales = forecasted_values$mean) %>%
full_join(., y = test, by = "date") %>%
select(id, sales)
# If first store-item save, otherwise bind
if(it == 1){
all_preds <- preds
} else {
all_preds <- bind_rows(all_preds, preds)
}
}
}
# Arrange predictions and write to a file
all_preds <- all_preds %>% arrange(id)
vroom_write(x = all_preds, file = "./submission.csv", delim = ",")
all_preds
# Import Libraries
library(tidyverse)
library(tidymodels)
library(modeltime)
library(timetk)
library(vroom)
library(forecast)
# Read in the data
item <- vroom::vroom("./train.csv")
itemTest <- vroom::vroom("./test.csv")
# Convert date to Date type
item$date <- as.Date(item$date)
itemTest$date <- as.Date(itemTest$date)
# Determine the number of stores and items
n.stores <- max(item$store)
n.items <- max(item$item)
# Initialize an empty data frame to store predictions
all_preds <- data.frame()
# Loop over all store-item combos
for(s in 1:n.stores) {
for(i in 1:n.items) {
# Subset the data for the current store-item combination
train <- item %>% filter(store == s, item == i)
test <- itemTest %>% filter(store == s, item == i)
# Check if there are any missing sales values
if(any(is.na(train$sales))) {
next # Skip to the next iteration if there are missing values
}
# Fit the SARIMA model
sarima_model <- auto.arima(train$sales, seasonal = TRUE)
# Forecast for the required horizon
forecast_horizon <- nrow(test)
forecasted_values <- forecast(sarima_model, h = forecast_horizon)
# Prepare the forecast for submission
preds <- data.frame(id = test$id, sales = forecasted_values$mean)
# Combine the predictions for each store-item combination
all_preds <- bind_rows(all_preds, preds)
}
}
all_preds
# Write the predictions to a CSV file
write.csv(all_preds, "sarima_predictions.csv", row.names = FALSE)
# Import Libraries
library(tidyverse)
library(tidymodels)
library(modeltime)
library(timetk)
library(vroom)
library(forecast)
# Read in the data
item <- vroom::vroom("./train.csv")
itemTest <- vroom::vroom("./test.csv")
# How many stores and items?
n.stores <- max(item$store)
n.items <- max(item$item)
it <- 0
# Double Loop over all store-item combos
for(s in 1:n.stores){
for(i in 1:n.items){
# Increment the progress bar
it <- it + 1
# Subset the data
train <- item %>%
filter(store == s, item == i) %>%
select(date, sales)
test <- itemTest %>%
filter(store == s, item == i) %>%
select(id, date)
# CV Split
cv_splits <- time_series_split(train, assess = "3 months", cumulative = TRUE)
# Define and Fit Exponential Smoothing Model
ets_model <- ets(training(cv_splits)$sales)
# Forecast using the fitted model
forecasted_values <- forecast(ets_model, h = length(testing(cv_splits)$sales))
# Prepare the forecast for submission
forecast_dates <- testing(cv_splits)$date
preds <- data.frame(date = forecast_dates, sales = forecasted_values$mean) %>%
full_join(., y = test, by = "date") %>%
select(id, sales)
# If first store-item save, otherwise bind
if(it == 1){
all_preds <- preds
} else {
all_preds <- bind_rows(all_preds, preds)
}
}
}
# Arrange predictions and write to a file
all_preds <- all_preds %>% arrange(id)
vroom_write(x = all_preds, file = "./submission.csv", delim = ",")
# Import Libraries
library(tidyverse)
library(tidymodels)
library(modeltime)
library(timetk)
library(vroom)
library(forecast)
# Read in the data
item <- vroom::vroom("path_to_your_train.csv")
# Import Libraries
library(tidyverse)
library(tidymodels)
library(modeltime)
library(timetk)
library(vroom)
library(forecast)
# Read in the data
item <- vroom::vroom("./train.csv")
itemTest <- vroom::vroom("./test.csv")
# Convert date to Date type
item$date <- as.Date(item$date)
itemTest$date <- as.Date(itemTest$date)
# Determine the number of stores and items
n.stores <- max(item$store)
n.items <- max(item$item)
# Initialize an empty data frame to store predictions
all_preds <- data.frame()
# Loop over all store-item combos
for(s in 1:n.stores) {
for(i in 1:n.items) {
# Subset the data for the current store-item combination
train <- item %>% filter(store == s, item == i)
test <- itemTest %>% filter(store == s, item == i)
# Check if there are any missing sales values
if(any(is.na(train$sales))) {
next # Skip to the next iteration if there are missing values
}
# Fit the Exponential Smoothing model
ets_model <- ets(train$sales)
# Forecast for the required horizon
forecast_horizon <- nrow(test)
forecasted_values <- forecast(ets_model, h = forecast_horizon)
# Prepare the forecast for submission
preds <- data.frame(id = test$id, sales = forecasted_values$mean)
# Combine the predictions for each store-item combination
all_preds <- bind_rows(all_preds, preds)
}
}
# Write the predictions to a CSV file
write.csv(all_preds, "exponential_smoothing_predictions.csv", row.names = FALSE)
